import sys
import os
import subprocess
import tempfile
from PySide6.QtWidgets import (
    QApplication, QMainWindow, QWidget,
    QLabel, QLineEdit, QTextEdit, QPushButton,
    QFileDialog, QVBoxLayout, QHBoxLayout, QGroupBox,
    QMessageBox, QStyleFactory, QProgressBar, QComboBox,
    QSpinBox, QCheckBox
)
from PySide6.QtCore import Qt, QThread, Signal, Slot
from docx import Document
import time
import json
import gc
import warnings
import re
import numpy as np
import torch
import logging

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings("ignore")

# –•–∞–∫ –¥–ª—è Windows
if sys.platform == "win32":
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["PYANNOTE_CACHE"] = os.path.expanduser("~/.cache/torch/pyannote")
    sys.modules['triton'] = None
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
try:
    from faster_whisper import WhisperModel

    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

try:
    import whisperx

    WHISPERX_AVAILABLE = True
except ImportError:
    WHISPERX_AVAILABLE = False
from packaging import version
# PyPI ¬´packaging¬ª :contentReference[oaicite:4]{index=4}
from importlib.metadata import version as pkg_version

try:
    WX_VER = pkg_version("whisperx")
except Exception:
    WX_VER = pkg_version("whisperx")
USE_ALIGNMENT = (
    pkg_version("whisperx") >= "3.4.0"   # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç safetensors
    and os.getenv("NO_ALIGNMENT", "0") != "1"
    and torch.cuda.is_available()        # —É—Å–∫–æ—Ä—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞GPU
)
ALIGN_MODELS = [
   "anton-l/wav2vec2-large-xlsr-53-russian",
   "pszemraj/wav2vec2-large-xlsr-53-russian-ru",
]



############################################################
# –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
############################################################

def clean_hallucinations(text):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    if not text or len(text) < 3:
        print(f"[DEBUG] –û—Ç–±—Ä–æ—à–µ–Ω–æ –∫–∞–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è: {text[:60]}‚Ä¶")
        return True


    # –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    hallucination_patterns = [
        r'[–æ–û–∞–ê—ç–≠—É–£—ã–´–∏–ò–µ–ï—ë–Å—é–Æ—è–Ø]{10,}',  # –î–ª–∏–Ω–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã –≥–ª–∞—Å–Ω—ã—Ö
        r'([–∞-—è–ê-–Ø])\1{5,}',  # –ü–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –æ–¥–Ω–æ–π –±—É–∫–≤—ã –±–æ–ª–µ–µ 5 —Ä–∞–∑
        r'(–æ–π|–∞–π|—ç–π|—É–π|–∏–π|–µ–π|—ë–π|—é–π|—è–π)[-\s]?\1{3,}',  # –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –º–µ–∂–¥–æ–º–µ—Ç–∏—è
        r'(\b\w{1,3}\b)(\s+\1){4,}',  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è –º–Ω–æ–≥–æ —Ä–∞–∑
        r'[\-]{5,}',  # –ú–Ω–æ–≥–æ –¥–µ—Ñ–∏—Å–æ–≤ –ø–æ–¥—Ä—è–¥
        r'[\.]{5,}',  # –ú–Ω–æ–≥–æ —Ç–æ—á–µ–∫ –ø–æ–¥—Ä—è–¥
        r'(\s|^)([—Ö–•]{3,}|[–ø–ü]{3,}|[—Ñ–§]{3,}|[—Å–°]{3,}|[—à–®]{3,})',  # –®–∏–ø—è—â–∏–µ/—Å–≤–∏—Å—Ç—è—â–∏–µ
    ]
    hallucination_patterns.extend([
        r'(?:\b[–æ–û]–π[,\s]*){4,}',  # ¬´–æ–π, –æ–π, –æ–π‚Ä¶¬ª
        r'(?:\b[–∞–ê]–π[,\s]*){4,}',  # ¬´–∞–π, –∞–π, –∞–π‚Ä¶¬ª
    ])

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    for pattern in hallucination_patterns:
        text = re.sub(pattern, '', text)

    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ—Å—Ç–æ—è—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ—Å—Ç–æ–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∏–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        if re.match(r'^(.{1,5})\1{3,}$', line):
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        if len(line) > 10:
            unique_chars = len(set(line.replace(' ', '')))
            total_chars = len(line.replace(' ', ''))
            if total_chars > 0 and unique_chars / total_chars < 0.2:  # –ú–µ–Ω–µ–µ 20% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
                continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if re.search(r'[–∞-—è–ê-–Ø]{20,}', line.replace(' ', '')):
            continue

        cleaned_lines.append(line)

    text = '\n'.join(cleaned_lines)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    text = re.sub(r'\s+', ' ', text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\n{3,}', '\n\n', text)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã

    return text.strip()


def is_likely_hallucination(text):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ–π –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–µ–π"""
    _log("DEBUG", f"–û—Ç–±—Ä–æ—à–µ–Ω–æ –∫–∞–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è: ¬´{text[:60]}‚Ä¶¬ª")
    if not text or len(text) < 3:
        return True
    text_lower = text.lower()  # ‚Üê –æ–±—ä—è–≤–ª—è–µ–º —Å—Ä–∞–∑—É
    tokens = re.findall(r'\b[–∞-—è–ê-–Ø]{1,4}\b', text_lower)
    if tokens and max(tokens.count(t) for t in set(tokens)) / len(tokens) > 0.4:
        return True

    # ‚â•50% –æ–¥–Ω–æ–π –±—É–∫–≤—ã
    if any(text_lower.count(ch) / len(text_lower) > 0.5 for ch in set(text_lower)):
        return True

    vowels = '–∞–µ—ë–∏–æ—É—ã—ç—é—è'
    consonants = '–±–≤–≥–¥–∂–∑–π–∫–ª–º–Ω–ø—Ä—Å—Ç—Ñ—Ö—Ü—á—à—â'
    return not (any(c in vowels for c in text_lower) and any(c in consonants for c in text_lower))

    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π –æ–¥–Ω–æ–π –±—É–∫–≤—ã
    for char in text_lower:
        if text_lower.count(char) / len(text_lower) > 0.5:
            return True

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≥–∏
    for i in range(1, min(4, len(text) // 2)):
        pattern = text[:i]
        if text == pattern * (len(text) // len(pattern)):
            return True

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–æ–≥–ª–∞—Å–Ω—ã—Ö –∏–ª–∏ –≥–ª–∞—Å–Ω—ã—Ö
    vowels = '–∞–µ—ë–∏–æ—É—ã—ç—é—è'
    consonants = '–±–≤–≥–¥–∂–∑–π–∫–ª–º–Ω–ø—Ä—Å—Ç—Ñ—Ö—Ü—á—à—â'

    has_vowel = any(c in vowels for c in text_lower)
    has_consonant = any(c in consonants for c in text_lower)

    if not has_vowel or not has_consonant:
        return True

    return False


############################################################
# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
############################################################
def _log(level, msg):
    (logger or (lambda l, m: print(f"[{l}] {m}")))(level, msg)
def transcribe_russian_optimized(file_path, model_size="large-v3", device="auto", hf_token=None,logger=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""



    # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å large –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å GPU
    if device == "cuda" and model_size in ["base", "small"]:
        print("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å 'large-v3' –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –Ω–∞ GPU")
    # –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è device
    _log("INFO", f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device.upper()} "
                     f"{torch.cuda.get_device_name(0) if device == 'cuda' else ''}")

    if device == "cpu":
        _log("WARNING",
                 "CPU‚Äë—Ä–µ–∂–∏–º: —Å–∫–æ—Ä–æ—Å—Ç—å ~‚Äë6√ó –º–µ–¥–ª–µ–Ω–Ω–µ–π GPU(—Å–º. –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏) "
                 "‚Äì –æ–∂–∏–¥–∞–π—Ç–µ –±–æ–ª–µ–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
    else:
        _log("INFO", "GPU‚Äë—Ä–µ–∂–∏–º: —É—Å–∫–æ—Ä–µ–Ω–∏–µ x5‚Äëx40 –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å CPU "
                         "–ø–æ –¥–∞–Ω–Ω—ã–º –æ—Ç–∫—Ä—ã—Ç—ã—Ö —Ç–µ—Å—Ç–æ–≤")  # :contentReference[oaicite:1]{index=1}
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º WhisperX –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    if WHISPERX_AVAILABLE and device == "cuda":
        return transcribe_with_whisperx_russian(file_path, model_size, device, hf_token)
    else:
        return transcribe_with_whisper_russian(file_path, model_size, device)


def transcribe_with_whisper_russian(file_path, model_size="base", device="cpu"):
    """Faster Whisper –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ"""

    compute_type = "float16" if device == "cuda" else "int8"

    # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    model = WhisperModel(
        model_size,
        device=device,
        compute_type=compute_type,
        download_root=os.path.expanduser("~/.cache/whisper")
    )

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    segments, info = model.transcribe(
        file_path,
        language="ru",
        task="transcribe",
        beam_size=15,
        best_of=5,
        patience=3,
        length_penalty=1,
        temperature=0.0,
        compression_ratio_threshold=2.0,
        log_prob_threshold=-0.7,
        no_speech_threshold=0.5,
        condition_on_previous_text=False,
        initial_prompt="–≠—Ç–æ —Ä–∞–∑–≥–æ–≤–æ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.",
        word_timestamps=True,
        prepend_punctuations="\"'¬ø([{-",
        append_punctuations="\"'.„ÄÇ,Ôºå!ÔºÅ?Ôºü:Ôºö",
        vad_filter=True,
        vad_parameters=dict(
            threshold=0.5,
            min_speech_duration_ms=250,
            max_speech_duration_s=9999,
            min_silence_duration_ms=800,
            window_size_samples=1024,
            speech_pad_ms=400
        )
    )

    all_segments = []
    for segment in segments:
        text = segment.text.strip()

        # –§–∏–ª—å—Ç—Ä—É–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
        if is_likely_hallucination(text):
            continue

        # –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
        cleaned_text = clean_hallucinations(text)

        if cleaned_text and len(cleaned_text) > 2:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–≥–º–µ–Ω—Ç–∞
            if hasattr(segment, 'avg_logprob') and segment.avg_logprob < -1.2:
                continue

            all_segments.append({
                'start': segment.start,
                'end': segment.end,
                'text': cleaned_text,
                'confidence': getattr(segment, 'avg_logprob', 0)
            })

    del model
    gc.collect()
    return all_segments


def transcribe_with_whisperx_russian(file_path, model_size="large-v3", device="cuda", hf_token=None):
    """WhisperX –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""

    batch_size = 16 if device == "cuda" else 4
    compute_type = "float16" if device == "cuda" else "int8"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
    audio = whisperx.load_audio(file_path)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
    model = whisperx.load_model(
        model_size,
        device,
        compute_type=compute_type,
        language="ru",
        asr_options={
            "beam_size": 10,
            "best_of": 5,
            "patience": 2,
            "length_penalty": 1,
            "temperatures": [0.0],
            "compression_ratio_threshold": 2.0,
            "log_prob_threshold": -0.7,
            "no_speech_threshold": 0.5,
            "condition_on_previous_text": False,
            "initial_prompt": "–≠—Ç–æ —Ä–∞–∑–≥–æ–≤–æ—Ä –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.",
            "suppress_tokens": [-1],
            "word_timestamps": True
        },
        vad_options={
            "vad_onset": 0.300,
            "vad_offset": 0.2
        }
    )

    # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
    result = model.transcribe(audio, batch_size=batch_size, language="ru")
    del model
    gc.collect()

    # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
    try:
        # –ü—Ä–æ–±—É–µ–º wav2vec2 –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
        align_model_names = [
            "anton-l/wav2vec2-large-xlsr-53-russian",
            "pszemraj/wav2vec2-large-xlsr-53-russian-ru",
        ]

        aligned = False
        for model_name in (ALIGN_MODELS if USE_ALIGNMENT else []):
            try:
                print(f"–ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª—å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: {model_name}")
                model_a, metadata = whisperx.load_align_model(
                    language_code="ru",
                    device=device,
                    model_name=model_name
                )

                result = whisperx.align(
                    result["segments"],
                    model_a,
                    metadata,
                    audio,
                    device,
                    return_char_alignments=False,
                )

                del model_a
                aligned = True
                print(f"–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å –º–æ–¥–µ–ª—å—é: {model_name}")
                break

            except Exception as e:
                print(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}")
                continue

        if not aligned:
            print("–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏")

    except Exception as e:
        print(f"–í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {e}")

    # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è
    if hf_token and "segments" in result:
        try:
            from pyannote.audio import Pipeline
            import torch

            print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
            diarize_model = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=hf_token
            )

            if device == "cuda":
                diarize_model.to(torch.device("cuda"))

            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            diarize_model.instantiate({
                "clustering": {
                    "method": "centroid",
                    "min_cluster_size": 15,
                    "threshold": 0.7,
                },
                "segmentation": {
                    "min_duration_off": 0.5,
                }
            })

            print("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏...")
            diarization = diarize_model(file_path)

            # –ù–∞–∑–Ω–∞—á–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    "start": turn.start,
                    "end": turn.end,
                    "speaker": speaker
                })

            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–µ–π
            for seg in result["segments"]:
                seg_mid = (seg.get("start", 0) + seg.get("end", 0)) / 2

                best_speaker = None
                best_overlap = 0

                for speaker_seg in speaker_segments:
                    overlap_start = max(seg.get("start", 0), speaker_seg["start"])
                    overlap_end = min(seg.get("end", 0), speaker_seg["end"])
                    overlap = max(0, overlap_end - overlap_start)

                    if overlap > best_overlap:
                        best_overlap = overlap
                        best_speaker = speaker_seg["speaker"]

                if best_speaker:
                    seg["speaker"] = best_speaker

            print("–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            print(f"–î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    final_segments = []
    for seg in result.get("segments", []):
        text = seg.get('text', '').strip()

        # –§–∏–ª—å—Ç—Ä—É–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
        if is_likely_hallucination(text):
            continue

        cleaned_text = clean_hallucinations(text)

        if cleaned_text and len(cleaned_text) > 2:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º confidence –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
            if 'confidence' in seg and seg['confidence'] < 0.5:
                continue

            final_segments.append({
                'start': seg.get('start', 0),
                'end': seg.get('end', 0),
                'text': cleaned_text,
                'speaker': seg.get('speaker'),
                'confidence': seg.get('confidence', 1.0)
            })

    if device == "cuda":
        try:
            import torch
            torch.cuda.empty_cache()
        except:
            pass

    return final_segments


############################################################
# –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è
############################################################

def apply_smart_diarization(segments, min_pause=1.5, merge_threshold=2.0):
    """–£–º–Ω–∞—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è —Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º —Å–µ–≥–º–µ–Ω—Ç–æ–≤"""
    if not segments:
        return []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
    has_speakers = any(seg.get('speaker') for seg in segments)

    if has_speakers:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
        diarized_segments = []
        speaker_mapping = {}
        speaker_count = 1

        for seg in segments:
            original_speaker = seg.get('speaker', 'SPEAKER_00')

            if original_speaker not in speaker_mapping:
                speaker_mapping[original_speaker] = f"–°–ø–∏–∫–µ—Ä {speaker_count}"
                speaker_count += 1

            diarized_segments.append({
                'speaker': speaker_mapping[original_speaker],
                'start': seg['start'],
                'end': seg['end'],
                'text': seg['text'],
                'confidence': seg.get('confidence', 1.0)
            })
    else:
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫—É—é –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
        diarized_segments = []
        current_speaker = 1
        last_end_time = 0

        for i, segment in enumerate(segments):
            pause = segment['start'] - last_end_time if i > 0 else 0

            # –ú–µ–Ω—è–µ–º —Å–ø–∏–∫–µ—Ä–∞ –ø—Ä–∏ –¥–ª–∏–Ω–Ω–æ–π –ø–∞—É–∑–µ
            if i > 0 and pause > min_pause:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –≤–æ–ø—Ä–æ—Å–∞,
                # –≤–µ—Ä–æ—è—Ç–Ω–æ –¥—Ä—É–≥–æ–π —Å–ø–∏–∫–µ—Ä
                if segment['text'].strip().endswith('?'):
                    current_speaker = 2 if current_speaker == 1 else 1
                elif pause > min_pause * 1.5:
                    current_speaker = 2 if current_speaker == 1 else 1

            diarized_segments.append({
                'speaker': f"–°–ø–∏–∫–µ—Ä {current_speaker}",
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'],
                'confidence': segment.get('confidence', 1.0)
            })
            last_end_time = segment['end']

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–ª–∏–∑–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
    merged_segments = []
    current_segment = None

    for seg in diarized_segments:
        if current_segment is None:
            current_segment = seg.copy()
            current_segment['texts'] = [current_segment['text']]
        elif (seg['speaker'] == current_segment['speaker'] and
              seg['start'] - current_segment['end'] < merge_threshold):
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º
            current_segment['end'] = seg['end']
            current_segment['texts'].append(seg['text'])
            # –ë–µ—Ä–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence
            current_segment['confidence'] = min(
                current_segment.get('confidence', 1.0),
                seg.get('confidence', 1.0)
            )
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–µ–≥–º–µ–Ω—Ç
            current_segment['text'] = ' '.join(current_segment['texts'])
            del current_segment['texts']
            merged_segments.append(current_segment)

            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
            current_segment = seg.copy()
            current_segment['texts'] = [current_segment['text']]

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
    if current_segment:
        current_segment['text'] = ' '.join(current_segment['texts'])
        del current_segment['texts']
        merged_segments.append(current_segment)

    return merged_segments


############################################################
# –†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫
############################################################

class TranscriptionWorker(QThread):
    progress_signal = Signal(int)
    status_signal = Signal(str, str)
    finished_signal = Signal(str, list)

    def __init__(self, video_file, language="ru", engine="whisper", model_size="base",
                 device="auto", min_pause=1.5, merge_threshold=2.0,
                 use_diarization=True, hf_token=None, parent=None):
        super().__init__(parent)
        self.video_file = video_file
        self.language = language
        self.engine = engine
        self.model_size = model_size
        self.device = device
        self.min_pause = min_pause
        self.merge_threshold = merge_threshold
        self.use_diarization = use_diarization
        self.hf_token = hf_token
        self.temp_dir = tempfile.TemporaryDirectory()
        self.output_audio_file = os.path.join(self.temp_dir.name, "extracted_audio.wav")


    def run(self):
        device_type = "GPU" if torch.cuda.is_available() else "CPU"        _log("INFO", f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_type} "
                         f"{torch.cuda.get_device_name(0) if device_type == 'GPU' else ''}")
        if device_type == "CPU":
            _log("WARNING", "CPU‚Äë—Ä–µ–∂–∏–º: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –±—É–¥–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –¥–æ–ª—å—à–µ.")
        try:
            start_time = time.time()
            _log("INFO", f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {os.path.basename(self.video_file)}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ FFmpeg
            if not self.check_ffmpeg():
                raise RuntimeError("FFmpeg –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–º–µ—Å—Ç–∏—Ç–µ ffmpeg.exe –≤ –ø–∞–ø–∫—É —Å –ø—Ä–æ–≥—Ä–∞–º–º–æ–π.")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –∞—É–¥–∏–æ
            _log("INFO", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥–∏–æ...")
            self.convert_audio(self.video_file, self.output_audio_file)
            self.progress_signal.emit(20)

            # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
            segments = []

            if self.language == "ru":
                # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
                _log("INFO", f"–ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞...")
                segments = transcribe_russian_optimized(
                    self.output_audio_file,
                    self.model_size,
                    self.device,
                    self.hf_token if self.use_diarization else None,
                    logger=_log
                )
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —è–∑—ã–∫–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é
                if self.engine == "whisperx" and WHISPERX_AVAILABLE:
                    _log("INFO", f"–ó–∞–ø—É—Å–∫ WhisperX...")
                    segments = transcribe_with_whisperx_russian(
                        self.output_audio_file,
                        self.model_size,
                        self.device,
                        self.hf_token if self.use_diarization else None
                    )
                    if USE_ALIGNMENT:
                        for model_name in ALIGN_MODELS:
                            try:
                                model_a, metadata = whisperx.load_align_model(
                                    language_code="ru",
                                    device=self.device,
                                    model_name=model_name
                                )
                                segments = whisperx.align(
                                    segments, model_a, metadata,
                                    whisperx.load_audio(self.output_audio_file),
                                    self.device, return_char_alignments=False
                                )
                                _log("INFO", f"Word‚Äëalignment ¬´{model_name}¬ª –≤—ã–ø–æ–ª–Ω–µ–Ω")
                                break
                            except Exception as e:
                                _log("WARNING", f"Word‚Äëalignment ¬´{model_name}¬ª –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
                    else:
                        _log("INFO", "Word‚Äëalignment –æ—Ç–∫–ª—é—á–µ–Ω")
                else:
                    _log("INFO", f"–ó–∞–ø—É—Å–∫ Whisper...")
                    segments = transcribe_with_whisper_russian(
                        self.output_audio_file,
                        self.model_size,
                        self.device
                    )

            self.progress_signal.emit(70)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
            if self.use_diarization:
                _log("INFO", "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏–∫–µ—Ä–æ–≤...")
                diarized_segments = apply_smart_diarization(
                    segments,
                    self.min_pause,
                    self.merge_threshold
                )
            else:
                # –ë–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ - –ø—Ä–æ—Å—Ç–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º
                diarized_segments = [{
                    'speaker': '–¢–µ–∫—Å—Ç',
                    'start': seg['start'],
                    'end': seg['end'],
                    'text': seg['text'],
                    'confidence': seg.get('confidence', 1.0)
                } for seg in segments]

            self.progress_signal.emit(90)

            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            filtered_segments = []
            for seg in diarized_segments:
                if seg.get('confidence', 1.0) > 0.3:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—á–µ–Ω—å –ø–ª–æ—Ö–∏–µ
                    filtered_segments.append(seg)

            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            formatted_text = self.format_diarized_text(filtered_segments)

            self.progress_signal.emit(95)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            end_time = time.time()
            duration = end_time - start_time
            unique_speakers = len(set(seg['speaker'] for seg in filtered_segments)) if filtered_segments else 0

            _log("SUCCESS", f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration:.1f} —Å–µ–∫")
            _log("INFO", f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(filtered_segments)}")
            if self.use_diarization:
                _log("INFO", f"–ù–∞–π–¥–µ–Ω–æ —Å–ø–∏–∫–µ—Ä–æ–≤: {unique_speakers}")

            self.progress_signal.emit(100)
            self.finished_signal.emit(formatted_text, filtered_segments)

        except Exception as e:
            import traceback
            error_msg = f"–û—à–∏–±–∫–∞: {str(e)}"
            _log("ERROR", error_msg)
            self.finished_signal.emit(error_msg, [])
            print(traceback.format_exc())
        finally:
            try:
                self.temp_dir.cleanup()
            except:
                pass

    def check_ffmpeg(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è FFmpeg"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            if os.path.exists('ffmpeg.exe'):
                return True
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ PATH
            result = subprocess.run(['ffmpeg', '-version'],
                                    capture_output=True, text=True)
            return result.returncode == 0
        except:
            return False

    def convert_audio(self, input_file, output_file):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏"""
        ffmpeg_cmd = self.get_ffmpeg_path()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–π —Ä–µ—á–∏
        cmd = [
            ffmpeg_cmd,
            "-y",
            "-i", input_file,
            "-vn",  # –£–±–∏—Ä–∞–µ–º –≤–∏–¥–µ–æ
            "-acodec", "pcm_s16le",  # 16-bit PCM
            "-ar", "16000",  # 16 kHz - –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è Whisper
            "-ac", "1",  # –ú–æ–Ω–æ
            "-af", "highpass=f=80,lowpass=f=8000,anlmdn=s=7:p=0.002",  # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è + —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
            output_file
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            raise RuntimeError(f"FFmpeg –æ—à–∏–±–∫–∞: {result.stderr}")

    def get_ffmpeg_path(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ FFmpeg"""
        if os.path.exists('ffmpeg.exe'):
            return os.path.abspath('ffmpeg.exe')
        return 'ffmpeg'

    def format_diarized_text(self, segments):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π"""
        lines = []
        for segment in segments:
            speaker = segment['speaker']
            text = segment['text']
            lines.append(f"{speaker}: {text}")
        return "\n\n".join(lines)

    def log(self, level, message):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ UI"""
        self.status_signal.emit(level, message)


############################################################
# –ì–ª–∞–≤–Ω–æ–µ –æ–∫–Ω–æ
############################################################

class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.transcribed_text = ""
        self.diarized_segments = []
        self.setWindowTitle("–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ v3.0")
        self.resize(1100, 800)
        self.setMinimumSize(900, 650)

        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        self.video_file_path = None
        self.transcription_thread = None

        self.init_ui(central_widget)
        self.check_requirements()

    def init_ui(self, central_widget):
        main_layout = QVBoxLayout(central_widget)

        # –ì—Ä—É–ø–ø–∞ –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–∞
        self.file_group = QGroupBox("1. –í—ã–±–æ—Ä —Ñ–∞–π–ª–∞")
        layout_file = QHBoxLayout()
        self.select_video_btn = QPushButton("–í—ã–±—Ä–∞—Ç—å –≤–∏–¥–µ–æ/–∞—É–¥–∏–æ")
        self.video_label = QLabel("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
        self.video_label.setWordWrap(True)
        layout_file.addWidget(self.select_video_btn)
        layout_file.addWidget(self.video_label, 1)
        self.file_group.setLayout(layout_file)

        # –ì—Ä—É–ø–ø–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        self.settings_group = QGroupBox("2. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏")
        settings_layout = QVBoxLayout()

        # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        main_settings = QHBoxLayout()

        # –Ø–∑—ã–∫ (—Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        lang_label = QLabel("–Ø–∑—ã–∫:")
        self.language_combo = QComboBox()
        self.language_combo.addItems(["–†—É—Å—Å–∫–∏–π", "English"])

        # –î–≤–∏–∂–æ–∫
        engine_label = QLabel("–î–≤–∏–∂–æ–∫:")
        self.engine_combo = QComboBox()
        if WHISPER_AVAILABLE:
            self.engine_combo.addItem("Whisper", "whisper")
        if WHISPERX_AVAILABLE:
            self.engine_combo.addItem("WhisperX", "whisperx")

        # –ú–æ–¥–µ–ª—å
        model_label = QLabel("–ú–æ–¥–µ–ª—å:")
        self.model_combo = QComboBox()
        self.model_combo.addItems([
            "tiny (39M)",
            "base (74M)",
            "small (244M)",
            "medium (769M)",
            "large-v3 (1550M)"
        ])
        self.model_combo.setCurrentText("small (244M)")

        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        device_label = QLabel("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ:")
        self.device_combo = QComboBox()
        self.device_combo.addItems(["–ê–≤—Ç–æ", "CPU", "GPU"])

        main_settings.addWidget(lang_label)
        main_settings.addWidget(self.language_combo)
        main_settings.addSpacing(20)
        main_settings.addWidget(engine_label)
        main_settings.addWidget(self.engine_combo)
        main_settings.addSpacing(20)
        main_settings.addWidget(model_label)
        main_settings.addWidget(self.model_combo)
        main_settings.addSpacing(20)
        main_settings.addWidget(device_label)
        main_settings.addWidget(self.device_combo)
        main_settings.addStretch()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        diarization_layout = QHBoxLayout()

        self.use_diarization = QCheckBox("–†–∞–∑–¥–µ–ª—è—Ç—å –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º")
        self.use_diarization.setChecked(True)

        pause_label = QLabel("–ú–∏–Ω. –ø–∞—É–∑–∞ (—Å–µ–∫):")
        self.pause_spin = QSpinBox()
        self.pause_spin.setMinimum(1)
        self.pause_spin.setMaximum(5)
        self.pause_spin.setValue(2)

        merge_label = QLabel("–û–±—ä–µ–¥–∏–Ω—è—Ç—å –µ—Å–ª–∏ < (—Å–µ–∫):")
        self.merge_spin = QSpinBox()
        self.merge_spin.setMinimum(1)
        self.merge_spin.setMaximum(5)
        self.merge_spin.setValue(2)

        diarization_layout.addWidget(self.use_diarization)
        diarization_layout.addSpacing(20)
        diarization_layout.addWidget(pause_label)
        diarization_layout.addWidget(self.pause_spin)
        diarization_layout.addSpacing(20)
        diarization_layout.addWidget(merge_label)
        diarization_layout.addWidget(self.merge_spin)
        diarization_layout.addStretch()

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        self.recommendation_label = QLabel(
            "üí° –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å 'large-v3' –Ω–∞ GPU"
        )
        self.recommendation_label.setStyleSheet("color: #1976d2; padding: 5px;")

        settings_layout.addLayout(main_settings)
        settings_layout.addLayout(diarization_layout)
        settings_layout.addWidget(self.recommendation_label)
        self.settings_group.setLayout(settings_layout)

        # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞
        self.transcribe_btn = QPushButton("3. –ù–∞—á–∞—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é")
        self.transcribe_btn.setStyleSheet("QPushButton { font-size: 14px; padding: 12px; }")

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä
        self.progress_bar = QProgressBar()
        self.progress_bar.hide()

        # –ì—Ä—É–ø–ø–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.result_group = QGroupBox("–†–µ–∑—É–ª—å—Ç–∞—Ç")
        result_layout = QVBoxLayout()
        self.result_text = QTextEdit()
        self.result_text.setReadOnly(True)

        # –ö–Ω–æ–ø–∫–∏ –¥–µ–π—Å—Ç–≤–∏–π
        buttons_layout = QHBoxLayout()
        self.report_btn = QPushButton("–°–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç")
        self.report_btn.setEnabled(False)
        self.save_btn = QPushButton("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
        self.save_btn.setEnabled(False)
        buttons_layout.addStretch()
        buttons_layout.addWidget(self.report_btn)
        buttons_layout.addWidget(self.save_btn)

        result_layout.addWidget(self.result_text)
        result_layout.addLayout(buttons_layout)
        self.result_group.setLayout(result_layout)

        # –ì—Ä—É–ø–ø–∞ –∂—É—Ä–Ω–∞–ª–∞
        _log_group = QGroupBox("–ñ—É—Ä–Ω–∞–ª")
        log_layout = QVBoxLayout()
        _log_text = QTextEdit()
        _log_text.setReadOnly(True)
        _log_text.setMaximumHeight(120)
        log_layout.addWidget(_log_text)
        _log_group.setLayout(log_layout)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π layout
        main_layout.addWidget(self.file_group)
        main_layout.addWidget(self.settings_group)
        main_layout.addWidget(self.transcribe_btn)
        main_layout.addWidget(self.progress_bar)
        main_layout.addWidget(self.result_group, 1)
        main_layout.addWidget(_log_group)

        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤
        self.select_video_btn.clicked.connect(self.select_video_file)
        self.transcribe_btn.clicked.connect(self.transcribe_video)
        self.report_btn.clicked.connect(self.create_report)
        self.save_btn.clicked.connect(self.save_results)
        self.language_combo.currentTextChanged.connect(self.on_language_changed)
        self.device_combo.currentTextChanged.connect(self.on_device_changed)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç–∏–ª—å
        QApplication.setStyle(QStyleFactory.create("Fusion"))
        self.setStyleSheet(self._build_stylesheet())

    def check_requirements(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        if not WHISPER_AVAILABLE and not WHISPERX_AVAILABLE:
            _log_text.append(
                '<div style="color: red;"><b>–û—à–∏–±–∫–∞!</b> –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∏ –æ–¥–∏–Ω –¥–≤–∏–∂–æ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏!</div>')
            self.transcribe_btn.setEnabled(False)
            return

        if WHISPER_AVAILABLE:
            _log_text.append('<div style="color: green;">‚úì Faster Whisper —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω</div>')
        if WHISPERX_AVAILABLE:
            _log_text.append('<div style="color: green;">‚úì WhisperX —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω</div>')

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
        try:
            import torch
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                _log_text.append(f'<div style="color: green;">‚úì GPU: {gpu_name}</div>')
                # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º large –º–æ–¥–µ–ª—å –¥–ª—è GPU
                self.model_combo.setCurrentText("large-v3 (1550M)")
            else:
                _log_text.append('<div style="color: orange;">‚ö† GPU –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU</div>')
        except ImportError:
            _log_text.append('<div style="color: orange;">‚ö† PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è GPU</div>')

    def on_language_changed(self, text):
        """–ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —è–∑—ã–∫–∞ –æ–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        if text == "–†—É—Å—Å–∫–∏–π":
            self.recommendation_label.setText(
                "üí° –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å 'large-v3' –Ω–∞ GPU"
            )
            self.recommendation_label.show()
        else:
            self.recommendation_label.hide()

    def on_device_changed(self, text):
        """–ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –æ–±–Ω–æ–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª–∏"""
        if text == "CPU":
            self.model_combo.setCurrentText("base (74M)")
            _log_text.append(
                '<div style="color: orange;">‚ö† –ù–∞ CPU —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å base –∏–ª–∏ small</div>')

    @Slot()
    def select_video_file(self):
        """–í—ã–±–æ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"""
        file_path, _ = QFileDialog.getOpenFileName(
            self, "–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª", "",
            "–í—Å–µ –º–µ–¥–∏–∞ (*.mp4 *.mov *.avi *.mkv *.webm *.mp3 *.wav *.m4a *.aac *.flac *.ogg *.opus);;–í–∏–¥–µ–æ (*.mp4 *.mov *.avi *.mkv *.webm);;–ê—É–¥–∏–æ (*.mp3 *.wav *.m4a *.aac *.flac *.ogg *.opus)"
        )
        if file_path:
            self.video_file_path = file_path
            self.video_label.setText(os.path.basename(file_path))

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ
            size_mb = os.path.getsize(file_path) / (1024 * 1024)
            duration = self.get_media_duration(file_path)

            info = f'<div>–§–∞–π–ª: {os.path.basename(file_path)}</div>'
            info += f'<div>–†–∞–∑–º–µ—Ä: {size_mb:.1f} –ú–ë</div>'
            if duration:
                info += f'<div>–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration}</div>'

            _log_text.append(info)
            self.result_text.clear()
            self.report_btn.setEnabled(False)
            self.save_btn.setEnabled(False)

    def get_media_duration(self, file_path):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ–¥–∏–∞—Ñ–∞–π–ª–∞"""
        try:
            cmd = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration',
                   '-of', 'default=noprint_wrappers=1:nokey=1', file_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                duration_sec = float(result.stdout.strip())
                minutes = int(duration_sec // 60)
                seconds = int(duration_sec % 60)
                return f"{minutes}:{seconds:02d}"
        except:
            pass
        return None

    @Slot()
    def transcribe_video(self):
        """–ó–∞–ø—É—Å–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"""
        if not self.video_file_path:
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏.")
            return

        # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.result_text.clear()
        _log_text.clear()
        self.report_btn.setEnabled(False)
        self.save_btn.setEnabled(False)

        _log_text.append("<b>üöÄ –ó–∞–ø—É—Å–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏...</b>")
        self.progress_bar.show()
        self.progress_bar.setValue(0)
        self.transcribe_btn.setEnabled(False)

        # –ú–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞—á–µ–Ω–∏–π
        lang_map = {"–†—É—Å—Å–∫–∏–π": "ru", "English": "en"}
        model_map = {
            "tiny (39M)": "tiny",
            "base (74M)": "base",
            "small (244M)": "small",
            "medium (769M)": "medium",
            "large-v3 (1550M)": "large-v3"
        }
        device_map = {"–ê–≤—Ç–æ": "auto", "CPU": "cpu", "GPU": "cuda"}

        # –°–æ–∑–¥–∞–µ–º —Ä–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫
        self.transcription_thread = TranscriptionWorker(
            self.video_file_path,
            language=lang_map.get(self.language_combo.currentText(), "ru"),
            engine=self.engine_combo.currentData() or "whisper",
            model_size=model_map.get(self.model_combo.currentText(), "base"),
            device=device_map.get(self.device_combo.currentText(), "auto"),
            min_pause=self.pause_spin.value(),
            merge_threshold=self.merge_spin.value(),
            use_diarization=self.use_diarization.isChecked(),
            hf_token=os.getenv("HF_TOKEN")
        )

        # –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã
        self.transcription_thread.progress_signal.connect(self.on_progress)
        self.transcription_thread.status_signal.connect(self.on_status)
        self.transcription_thread.finished_signal.connect(self.on_transcription_finished)

        # –ó–∞–ø—É—Å–∫–∞–µ–º
        self.transcription_thread.start()

    @Slot(int)
    def on_progress(self, value):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–∞"""
        self.progress_bar.setValue(value)

    @Slot(str, str)
    def on_status(self, level, message):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        colors = {
            "ERROR": "#ffebee",
            "WARNING": "#fff3e0",
            "SUCCESS": "#e8f5e9",
            "INFO": "#e3f2fd"
        }
        color = colors.get(level, "#f5f5f5")

        icons = {
            "ERROR": "‚ùå",
            "WARNING": "‚ö†Ô∏è",
            "SUCCESS": "‚úÖ",
            "INFO": "‚ÑπÔ∏è"
        }
        icon = icons.get(level, "")

        _log_text.append(
            f'<div style="background: {color}; color: #212121; padding: 4px 8px; '
            f'border-radius: 4px; margin: 2px 0;">'
            f'{icon} <b>[{level}]</b> {message}</div>'
        )

    @Slot(str, list)
    def on_transcription_finished(self, result_text, segments):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"""
        self.progress_bar.hide()
        self.transcribe_btn.setEnabled(True)

        if result_text.startswith("–û—à–∏–±–∫–∞:"):
            _log_text.append(f'<div style="color: red;"><b>{result_text}</b></div>')
            QMessageBox.critical(self, "–û—à–∏–±–∫–∞", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏. –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ –≤ –∂—É—Ä–Ω–∞–ª–µ.")
        else:
            self.transcribed_text = result_text
            self.diarized_segments = segments
            self.result_text.setHtml(self._format_html_text(result_text))
            _log_text.append(
                '<div style="background: #e8f5e9; color: #212121; padding: 6px;">'
                '<b>‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!</b></div>'
            )
            self.report_btn.setEnabled(True)
            self.save_btn.setEnabled(True)

        self.transcription_thread = None

    def _format_html_text(self, text):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        html_lines = []
        colors = {
            "–°–ø–∏–∫–µ—Ä 1": "#1976d2",
            "–°–ø–∏–∫–µ—Ä 2": "#388e3c",
            "–°–ø–∏–∫–µ—Ä 3": "#d32f2f",
            "–°–ø–∏–∫–µ—Ä 4": "#7b1fa2",
            "–°–ø–∏–∫–µ—Ä 5": "#f57c00",
            "–¢–µ–∫—Å—Ç": "#424242"
        }

        for line in text.split('\n\n'):
            if ':' in line:
                try:
                    speaker, content = line.split(':', 1)
                    color = colors.get(speaker.strip(), "#424242")
                    html_lines.append(
                        f'<p style="margin: 10px 0; line-height: 1.5;">'
                        f'<b style="color:{color};">{speaker}:</b> {content.strip()}'
                        f'</p>'
                    )
                except ValueError:
                    html_lines.append(f'<p style="margin: 5px 0;">{line}</p>')
            elif line.strip():
                html_lines.append(f'<p style="margin: 5px 0;">{line}</p>')

        return "".join(html_lines)

    def create_report(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"""
        if not self.transcribed_text:
            return

        report = [
            "–û–¢–ß–ï–¢ –û –¢–†–ê–ù–°–ö–†–ò–ë–ê–¶–ò–ò",
            "=" * 50,
            f"\n–§–∞–π–ª: {os.path.basename(self.video_file_path)}",
            f"–î–∞—Ç–∞: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"–î–≤–∏–∂–æ–∫: {self.engine_combo.currentText()}",
            f"–ú–æ–¥–µ–ª—å: {self.model_combo.currentText()}",
            f"–Ø–∑—ã–∫: {self.language_combo.currentText()}",
            f"\n{'=' * 50}",
            "–°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n"
        ]

        if self.diarized_segments:
            speaker_stats = {}
            total_words = 0
            total_duration = 0
            total_confidence = 0
            confidence_count = 0

            for seg in self.diarized_segments:
                speaker = seg['speaker']
                words = len(seg['text'].split())
                duration = seg['end'] - seg['start']
                confidence = seg.get('confidence', 1.0)

                total_words += words
                total_duration = max(total_duration, seg['end'])
                if 'confidence' in seg:
                    total_confidence += confidence
                    confidence_count += 1

                if speaker not in speaker_stats:
                    speaker_stats[speaker] = {
                        'words': 0,
                        'segments': 0,
                        'duration': 0,
                        'confidence_sum': 0,
                        'confidence_count': 0
                    }

                speaker_stats[speaker]['words'] += words
                speaker_stats[speaker]['segments'] += 1
                speaker_stats[speaker]['duration'] += duration
                if 'confidence' in seg:
                    speaker_stats[speaker]['confidence_sum'] += confidence
                    speaker_stats[speaker]['confidence_count'] += 1

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
            for speaker, stats in sorted(speaker_stats.items()):
                percent_words = (stats['words'] / total_words * 100) if total_words > 0 else 0
                percent_time = (stats['duration'] / total_duration * 100) if total_duration > 0 else 0

                report.append(f"{speaker}:")
                report.append(f"  –†–µ–ø–ª–∏–∫: {stats['segments']}")
                report.append(f"  –°–ª–æ–≤: {stats['words']} ({percent_words:.1f}%)")
                report.append(f"  –í—Ä–µ–º—è —Ä–µ—á–∏: {stats['duration']:.1f} —Å–µ–∫ ({percent_time:.1f}%)")

                if stats['confidence_count'] > 0:
                    avg_confidence = stats['confidence_sum'] / stats['confidence_count']
                    report.append(f"  –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.2%}")

                report.append("")

            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            report.append("–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            report.append(f"  –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
            report.append(f"  –°–ø–∏–∫–µ—Ä–æ–≤: {len(speaker_stats)}")
            report.append(f"  –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {total_duration:.1f} —Å–µ–∫ ({total_duration / 60:.1f} –º–∏–Ω)")

            if confidence_count > 0:
                avg_total_confidence = total_confidence / confidence_count
                report.append(f"  –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_total_confidence:.2%}")

        report.extend(["\n" + "=" * 50, "–¢–†–ê–ù–°–ö–†–ò–ü–¶–ò–Ø:", "=" * 50 + "\n", self.transcribed_text])

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        file_path, _ = QFileDialog.getSaveFileName(
            self, "–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç",
            f"report_{time.strftime('%Y%m%d_%H%M%S')}.txt",
            "Text Files (*.txt)"
        )

        if file_path:
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(report))
                QMessageBox.information(self, "–£—Å–ø–µ—Ö", "–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
            except Exception as e:
                QMessageBox.critical(self, "–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç: {e}")

    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        if not self.transcribed_text:
            return

        file_path, _ = QFileDialog.getSaveFileName(
            self, "–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç",
            f"transcription_{time.strftime('%Y%m%d_%H%M%S')}",
            "Word (*.docx);;Text (*.txt);;JSON (*.json);;SRT (*.srt);;VTT (*.vtt)"
        )

        if file_path:
            try:
                if file_path.endswith('.docx'):
                    self._save_docx(file_path)
                elif file_path.endswith('.json'):
                    self._save_json(file_path)
                elif file_path.endswith('.srt'):
                    self._save_srt(file_path)
                elif file_path.endswith('.vtt'):
                    self._save_vtt(file_path)
                else:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(self.transcribed_text)

                QMessageBox.information(self, "–£—Å–ø–µ—Ö", "–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
            except Exception as e:
                QMessageBox.critical(self, "–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª: {e}")

    def _save_docx(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Word"""
        doc = Document()
        doc.add_heading('–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è', 0)

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        doc.add_paragraph(
            f'–§–∞–π–ª: {os.path.basename(self.video_file_path)}\n'
            f'–î–∞—Ç–∞: {time.strftime("%Y-%m-%d %H:%M:%S")}\n'
            f'–ú–æ–¥–µ–ª—å: {self.model_combo.currentText()}\n'
        )

        doc.add_heading('–¢–µ–∫—Å—Ç', level=1)

        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è
        for line in self.transcribed_text.split('\n\n'):
            if ':' in line:
                try:
                    speaker, content = line.split(':', 1)
                    p = doc.add_paragraph()
                    p.add_run(f"{speaker}: ").bold = True
                    p.add_run(content.strip())
                except ValueError:
                    doc.add_paragraph(line)
            elif line.strip():
                doc.add_paragraph(line)

        doc.save(path)

    def _save_json(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON"""
        data = {
            'metadata': {
                'file': os.path.basename(self.video_file_path),
                'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'engine': self.engine_combo.currentText(),
                'model': self.model_combo.currentText(),
                'language': self.language_combo.currentText()
            },
            'segments': [
                {
                    'speaker': seg['speaker'],
                    'start': seg['start'],
                    'end': seg['end'],
                    'text': seg['text'],
                    'confidence': seg.get('confidence', 1.0)
                }
                for seg in self.diarized_segments
            ],
            'full_text': self.transcribed_text
        }

        with open(path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _save_srt(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ SRT (—Å—É–±—Ç–∏—Ç—Ä—ã)"""
        with open(path, 'w', encoding='utf-8') as f:
            for i, seg in enumerate(self.diarized_segments):
                start = self._time_to_srt(seg.get('start', 0))
                end = self._time_to_srt(seg.get('end', 0))
                text = f"{seg['speaker']}: {seg['text']}" if self.use_diarization.isChecked() else seg['text']
                f.write(f"{i + 1}\n{start} --> {end}\n{text}\n\n")

    def _save_vtt(self, path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ WebVTT"""
        with open(path, 'w', encoding='utf-8') as f:
            f.write("WEBVTT\n\n")
            for i, seg in enumerate(self.diarized_segments):
                start = self._time_to_vtt(seg.get('start', 0))
                end = self._time_to_vtt(seg.get('end', 0))
                text = f"{seg['speaker']}: {seg['text']}" if self.use_diarization.isChecked() else seg['text']
                f.write(f"{start} --> {end}\n{text}\n\n")

    def _time_to_srt(self, seconds):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç SRT"""
        h, rem = divmod(seconds, 3600)
        m, s = divmod(rem, 60)
        ms = int((s - int(s)) * 1000)
        return f"{int(h):02d}:{int(m):02d}:{int(s):02d},{ms:03d}"

    def _time_to_vtt(self, seconds):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç VTT"""
        h, rem = divmod(seconds, 3600)
        m, s = divmod(rem, 60)
        ms = int((s - int(s)) * 1000)
        return f"{int(h):02d}:{int(m):02d}:{int(s):02d}.{ms:03d}"

    def _build_stylesheet(self):
        """–°—Ç–∏–ª—å –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        return """
            QMainWindow, QWidget {
                background: #fafafa;
                font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
                color: #212121;
            }
            QGroupBox {
                font-weight: 600;
                background: white;
                border: 1px solid #e0e0e0;
                border-radius: 8px;
                margin-top: 10px;
                padding: 15px;
            }
            QGroupBox::title {
                subcontrol-origin: margin;
                padding: 0 10px;
                color: #1976d2;
            }
            QLabel {
                color: #424242;
                padding-top: 5px;
            }
            QPushButton {
                background: #1976d2;
                color: white;
                border: none;
                padding: 10px 22px;
                border-radius: 4px;
                font-weight: 500;
                font-size: 13px;
            }
            QPushButton:hover {
                background: #1565c0;
            }
            QPushButton:pressed {
                background: #0d47a1;
            }
            QPushButton:disabled {
                background: #bdbdbd;
            }
            QComboBox, QSpinBox {
                padding: 6px 10px;
                border: 1px solid #e0e0e0;
                border-radius: 4px;
                background: white;
                color: #212121;
                min-width: 100px;
            }
            QComboBox:focus, QSpinBox:focus {
                border-color: #1976d2;
            }
            QTextEdit {
                border: 1px solid #e0e0e0;
                border-radius: 4px;
                background: white;
                padding: 8px;
                color: #212121;
                font-size: 13px;
                line-height: 1.4;
            }
            QProgressBar {
                border: none;
                border-radius: 4px;
                background: #e0e0e0;
                height: 20px;
                text-align: center;
                color: white;
            }
            QProgressBar::chunk {
                background: #1976d2;
                border-radius: 4px;
            }
            QCheckBox {
                padding: 5px;
            }
            QCheckBox::indicator {
                width: 18px;
                height: 18px;
            }
        """

    def closeEvent(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
        if self.transcription_thread and self.transcription_thread.isRunning():
            reply = QMessageBox.question(
                self, '–ó–∞–∫—Ä—ã—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É',
                '–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –µ—â–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è. –í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ –≤—ã–π—Ç–∏?',
                QMessageBox.Yes | QMessageBox.No,
                QMessageBox.No
            )

            if reply == QMessageBox.Yes:
                self.transcription_thread.quit()
                self.transcription_thread.wait(5000)
                event.accept()
            else:
                event.ignore()
        else:
            event.accept()


def main():
    app = QApplication(sys.argv)
    app.setApplicationName("Whisper Transcription RU")
    app.setOrganizationName("WhisperRU")

    window = MainWindow()
    window.show()

    sys.exit(app.exec())


if __name__ == "__main__":
    main()
